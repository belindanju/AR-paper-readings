{"config":{"lang":["ja"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>Hello, my name is Tian Guo I am a computer science professor at WPI.  This site tracks interesting developments and research on augmented reality (AR), as well as my personal reading notes. </p>"},{"location":"#why-this-site","title":"Why This Site?","text":"<p>A large part of my job is research, which roughly divides into keeping up with SToA, advising students, hands-on projects, and writing grant proposals. </p> <p>To officially kick start my NSF CAREER project Toward a Specialized Edge for Augmented Reality, I will spend some parts of my sabbatical year to organize impactful works in the field of AR. </p>"},{"location":"#relevant-conferences","title":"Relevant Conferences","text":"<p>Here are the list of conferences, not in any particular order, which I find each paper. </p> <ul> <li>MobiSys </li> <li>MobiCom </li> <li>IEEE VR </li> <li>IPSN </li> <li>SenSys </li> <li>CVPR </li> <li>ECCV </li> </ul>"},{"location":"benchmarks/Huzaifa2020-hd/","title":"Exploring Extended Reality with ILLIXR: A New Playground for Architecture Research","text":""},{"location":"benchmarks/Huzaifa2020-hd/#overview","title":"Overview","text":"<p>Huzaifa, M. et al. 2020. Exploring Extended Reality with ILLIXR: A New Playground for Architecture Research. arXiv [cs.DC].</p>"},{"location":"benchmarks/Huzaifa2020-hd/#how-many-passes","title":"How Many Passes?","text":""},{"location":"benchmarks/Huzaifa2020-hd/#my-ratings","title":"My Ratings","text":"<ul> <li>novelty:  </li> <li>readability:  </li> <li>reproducability:  </li> <li>practicability:  </li> </ul>"},{"location":"benchmarks/Huzaifa2020-hd/#high-level-ideas","title":"High-Level Ideas","text":"<p>ILLIXR consists of an open-source design of three pipelines. </p> <ul> <li>ILLIXR can support running typical XR applications. Though it seems that these XR applications have to be developed based on Monado. </li> </ul> <p>The authors' intention (at least original intention) is to target the computer architecture community.  In essence, the authors envisioned ILLIXR to serve as an open-source benchmark that has reference implementations for the entire XR workflow.</p> <p>ILLIXR is envisioned to serve as what the authors referred to as a full system testbed. </p> <p>The design and implementation of ILLIXR addressed two main challenges </p> <ul> <li>identifying a representative XR workflow and includes many SToA algorithms for different components </li> <li>has an OpenXR compliant implementation</li> </ul> <p>Figure 1 very nicely captures the essence of ILLIXR. One thing to note is that for an XR application to use ILLIXR, the application has to use OpenXR API. Consequently, application performance data can be measured at the ILLIXR-level.</p> <p> </p> <p>It might also be useful to compare ILLIXR to the OpenXR. IMO, they are quite similar and it would be useful to come back to understand the key differences later.</p> <p>Figure 2 is a very nice diagram that illustrates the complexity of XR application scheduling.  </p> <p>This reminds me of works that try to schedule the XR applications to deliver good user experiences, e.g., Heimdall, Layercake (our own work), and Linkshare. The key challenges comes down to how to schedule with dependencies. </p>"},{"location":"benchmarks/Huzaifa2020-hd/#key-novelties","title":"Key Novelties","text":"<p>An end-to-end modularized XR reference implementation! Most, if not all, components of ILLIXR are based on the existing works, which is a fine starting point.</p> <p>A good introduction to the XR pipeline and a useful background reading.</p> <p>Unlike a lot of AR works, including our own, that only focus on perception and visual results, ILLIXR also includes an audio pipeline.</p>"},{"location":"benchmarks/Huzaifa2020-hd/#evaluation-highlights","title":"Evaluation Highlights","text":"<p>It is cool that ILLIXR supports many performance metrics, including one called FLIP which I had never heard before but looks promising. In our own work, we had heavily relied on PSNR and SSIM but we definitely have noticed the limited expressiveness of those two metrics regarding human perception.  </p> <p>The authors also presented an evaluation using ILLIXR, on three different hardware platforms including an NVIDIA Jetson AGX Xavier development board. I am excited to find out if I can replicate some of the experiments on my Xavier board! </p> <p>It is nice to see ILLIXR's capability to help evaluate different XR applications. Though the evaluation results are not very surprising. </p>"},{"location":"benchmarks/Huzaifa2020-hd/#questions","title":"Questions","text":"<p>What are the key differences between Monado and ILLIXR? </p> <p>It is unclear how ILLIXR synchronizes the three pipelines. There are some discussions in Sec 3.2 Runtime about execution dependencies, though they are not quite the same thing. </p>"},{"location":"benchmarks/Huzaifa2020-hd/#a-closing-thought","title":"A Closing Thought","text":"<p>Overall I enjoyed reading this paper and have learnt something new. I admire the authors' efforts to support XR research community and I also share their ambitions! </p>"},{"location":"benchmarks/Huzaifa2020-hd/#references","title":"References","text":"<p>Yi, J. and Lee, Y. 2020. Heimdall: mobile GPU coordination platform for augmented reality applications. Proceedings of the 26th Annual International Conference on Mobile Computing and Networking (New York, NY, USA, Sep. 2020), 1\u201314.</p> <p>Hu, B. and Hu, W. 2019. LinkShare: device-centric control for concurrent and continuous mobile-cloud interactions. Proceedings of the 4th ACM/IEEE Symposium on Edge Computing (Nov. 2019), 15\u201329.</p> <p>Ogden, S. and Guo, T. 2023. LayerCake: Efficient Inference Serving with Cloud and Mobile Resources. THE 23rd IEEE/ACM International Symposium On Cluster, Cloud and Internet Computing (CCGrid'23).</p>"},{"location":"datasets/Yu2023-oz/","title":"MVImgNet: A Large-scale Dataset of Multi-view Images","text":""},{"location":"datasets/Yu2023-oz/#overview","title":"Overview","text":"<p>Yu, X. et al. 2023. MVImgNet: A Large-scale Dataset of Multi-view Images. CVPR.</p>"},{"location":"datasets/Yu2023-oz/#how-many-passes","title":"How Many Passes?","text":""},{"location":"datasets/Yu2023-oz/#my-ratings","title":"My Ratings","text":"<ul> <li>novelty:  </li> <li>readability:  </li> <li>reproducability:  </li> <li>practicability:  </li> </ul>"},{"location":"datasets/Yu2023-oz/#high-level-ideas","title":"High-Level Ideas","text":"<p>A large scale dataset that consists of multi-view images.</p> <ul> <li>Why? Provide a generic dataset for 3D vision. The authors argue that other 3D datasets have limitations such as lacking real-world cues. </li> <li>MVImgNet consists of 220k objects spanning 238 categories. </li> <li>Capturing process: </li> <li>Videos captured by mobile devices </li> <li>Videos need to meet certain criterial: <ul> <li>constraints on the videos: 0 seconds, not blurred, and each video has a dominant object </li> <li>constraints on objects: objects need to be stereoscopic, at least 180 degree view of the object; and the proportional of the object should be at least 15% in the video </li> </ul> </li> <li>Videos were captured by about 1000 collectors and the capturing quality is ensured by data cleaners </li> </ul>"},{"location":"datasets/Yu2023-oz/#key-novelties","title":"Key Novelties","text":"<p>A large-scale open source dataset. </p> <p>The authors investigate the benefits of using MVImgNet dataset on a few vision tasks. </p> <ul> <li>Radiance field reconstruction</li> <li>Multi-View Stereo: I am not convinced that MVImgNet helped much in this case. </li> </ul>"},{"location":"datasets/Yu2023-oz/#radiance-field-reconstruction","title":"Radiance field reconstruction","text":"<p>The authors chose a recent work called IBRNet and trained it in three different ways: from scratch, pre-train with CO3D, and pre-train with MVImgNet. I guess I am not surprised to see that IBRNet trained with MVImgNet achieves better results. It is a much larger dataset (220k objects and 238 categories) compared to CO3D (100 objects from 56 categories).</p> <p>In fact, if we look at the Table 4 and Figure 5 from the paper, we see that:</p> <ul> <li>Compared to CO3D, MVImgNet-small, which is a random subset of the original dataset,  does not seem to improve the three metrics much.</li> <li>The impact on the PSNR metric is quite expensive: increasing the training dataset by an order of magnitude brings about 1dB to 2dB improvement in PSNR. </li> </ul> <p></p> <p></p> <p>A couple of things I wish the authors have explained are: - How many times the experiments were repeated for the randomly generated subset?  - How would different sampling methods impact the evaluation results?</p>"},{"location":"datasets/Yu2023-oz/#multi-view-stereo","title":"Multi-View Stereo","text":"<p>There are a few things that might be worthy revisiting:</p> <ul> <li>How exactly was MVImgNet used to train JDACS? </li> <li>Table 5 shows that the depth map accuracy improves as the DTU training samples increase. This results are intuitive. What is interesting is that the accuracy gap between CO3D and MVImgNet-small is in favor of CO3D. If we look at Table 6, for both 4mm and 8mm error thresholds, JDACS trained with CO3D has higher accuracy. The accuracy improvement brought by MVImgNet also seems trivial. For example, training with MVImgNet only increases the accuracy from 78.39 (training with CO3D) to 78.58 while the dataset size difference is almost three order-of-magnitude. </li> </ul>"},{"location":"datasets/Yu2023-oz/#questions","title":"Questions","text":"<p>It seems that the camera intrinsic and extrinsic parameters is reconstructed from the videos. If a downstream task relied on these parameters, what are impacts on the task accuracy? </p> <ul> <li>Why didn't the authors collect those camera parameters? </li> </ul> <p>The dense depth maps are also reconstructed using a general-purpose pipeline called COLMAP, rather than measured. As such, this dataset is probably not suitable for depth estimation related tasks.</p>"},{"location":"datasets/Yu2023-oz/#a-closing-thought","title":"A Closing Thought","text":"<p>I always like works that can help promote research efforts in the community. A large-scale dataset definitely fits the bell. Though I wish some of the important scene data were captured instead of inferred. </p>"}]}